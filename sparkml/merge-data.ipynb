{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Merged Dataset\n",
    "\n",
    "In this workbook, you will read in the `trip` and `fare` files. You are welcome to use DataFrame and/or SparkSQL API as you desire as long as it produces the expected results.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "1. Join both datasets such that you get a merged dataset with 21 unique fields. You need to determine how to join the dataset.\n",
    "2. Once you create the merged dataset, you need to convert fields to the following types, since all fields were read is as string:\n",
    "    * pickup_datetime and dropoff_datetime must be TIMESTAMP\n",
    "    * passenger_count and rate_code must be INT\n",
    "    * all other numeric fields must be FLOAT\n",
    "    * the remaining fields stay as STRING\n",
    "3. Save your merged and converted dataset to your own S3 bucket in parquet format.\n",
    "\n",
    "You are welcome to add as many cells as you need below up until the next section. **You must include comments in your code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sparkml\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://novacationdembp.fios-router.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1051cf6d8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load trip file into a DataFrame\n",
    "trip_df = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('s3://bigdatateaching/nyctaxi-2013/parquet/trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fare file into a DataFrame\n",
    "fare_df = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('s3://bigdatateaching/nyctaxi-2013/parquet/fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join trip and fare dataframe on columns 'medallion', ' hack_license' and 'vendor_id'\n",
    "merged_df=trip_df.join(fare_df, (trip_df.medallion == fare_df.medallion)\n",
    "                    & (trip_df.hack_license == fare_df.hack_license)\n",
    "                    & (trip_df.vendor_id == fare_df.vendor_id)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pickup_datetime and dropoff_datetime from string to TIMESTAMP\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "merged_df= merged_df.withColumn('pickup_datetime',\n",
    "                                unix_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd hh:mm:ss\").cast(\"timestamp\"))\n",
    "merged_df= merged_df.withColumn(\"dropoff_datetime\", \n",
    "                                unix_timestamp(col(\"dropoff_datetime\"),\"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert passenger_count and rate_code from string to INT\n",
    "from pyspark.sql.types import IntegerType\n",
    "merged_df = merged_df.withColumn(\"passenger_count\", merged_df[\"passenger_count\"].cast(IntegerType()))\n",
    "merged_df = merged_df.withColumn(\"rate_code\", merged_df[\"rate_code\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all other numeric fields from string to float\n",
    "for col_name in cols:\n",
    "    df = df.withColumn(col_name, col(col_name).cast('float')).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cells, please provide the requested code and output. Do not change the order and/or structure of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, provide the code that saves your merged dataset to your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your merged and converted dataset to your own S3 bucket in parquet format\n",
    "import pyarrow.parquet as pq\n",
    "pq.write_table(merged_df, 's3://bigdata6278exercise/merged.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the schema of your merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the first 10 records of your merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the row count of your merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.count()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
